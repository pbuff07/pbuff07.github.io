---
title: Spark基础
date: 2025-01-11 11:10:01
categories: [Spark]
tags: ['spark', 'program']     # TAG names should always be lowercase
---



# Spark

参考文档

> https://spark.apache.org/docs/3.5.1/sql-ref-datetime-pattern.html#content



#### spark读取数据

spark可以使用spark.read.load加载各类数据源

```
df = spark.read.load("examples//users.parquet")
```

支持的数据源有：json, parquet, jdbc, orc, libsvm, csv, text

```
var pf = spark.read.parquet("x x x x x")
```



#### spark创建临时视图便于查询

有个疑问是否这些数据源都可以通过读取之后创建临时视图并进行spark sql查询呢？

> pf.createOrReplaceTempView("http")
>

```
经过测试大都是是可以的：比如parquet、json

经过测试大都是可以创建临时视图的，但是没法查询，比如text创建临时视图后查询结果就只有value一列，没办法进行where条件筛选。要筛选要么进行行的判断要么进行切割。

text类型通过txt_res.printSchema()查看文件结构就会如下，只有一个字段，没法进行筛选匹配。那么推理csv格式化的数据也应该是这样的。只要和text文本具有相同属性的应该都是这样的。
scala> txt_res.printSchema()
root
 |-- value: string (nullable = true)

```

#### 读取parquet数据并使用spark.sql查询

```
var pf = spark.read.parquet("/data/xx_data")
pf.createOrReplaceTempView("http")

spark.sql("select * from http limit 2").coalesce(1024).write.parquet("/tmp/xx_data_res")
```

#### spark.sql查询结果显示

默认使用.show()只会显示前20条

```
spark.sql("xxxxx").show()
```

如果想全部显示

```
spark.sql("xxxx").collect().foreach(println)
```

#### spark.sql结果输出不同文件格式

parquet

```
spark.sql("xxx").coalesce(1024).write.parquet("/tmp/xx_data_res")
```

csv

```
spark.sql("xxx").coalesce(1024).write.format("csv").save("/tmp/xx_data_res")
```

json

```
spark.sql("xxx").coalesce(1024).write.format("json").save("/tmp/xx_data_res")
```

#### .coalesce()操作

`coalesce` 是一个用于重新分区（reduce the number of partitions）的操作。它可以减少数据集的分区数量，这在需要减少分区以优化性能或资源使用时非常有用。

通过减少分区数量，可以减少任务启动的开销。对于大数据集，分区过多会导致大量的小任务启动，从而增加任务调度的开销。将分区数量减少到合理范围可以提高性能。

使用下面的命令将数据重新分区为1024个分区。

```
val coalescedDF = df.coalesce(1024)
```



#### spark.sql中常见函数（用到就记录）

按照特定的日期格式处理字段

```
select date_format(start_time, 'yyyy-MM-dd')
```



#### spark读取多个文件数据

> 可直接用*全部指定或者{}部分指定

```
var pf = spark.read.parquet("/tmp/xx_data_res/*")

var pf = spark.read.parquet("/tmp/xx_data_res/{month=1,month=2,month=3}")
```



有个疑问有时候json可以直接创建临时试图进行sql查询，为什么不直接使用而是读取之后转为parquet文件再进行读取后进行sql查询操作呢？

参考：https://spark.apache.org/docs/latest/sql-data-sources-parquet.html

```
性能： Parquet 是一种列式存储格式，它在读取和查询大型数据集时通常比 JSON 格式更高效。Parquet 具有更好的压缩率和更快的读取性能，尤其在处理大量数据时。如果你的数据集较小，性能差异可能不太明显。

列式存储优势： Parquet 以列式存储的方式组织数据，这使得查询时只需读取需要的列，而不是整个记录。这在 SQL 查询中可以带来更好的性能，因为它避免了读取不相关的数据。

谓词下推： Parquet 文件支持谓词下推，这意味着查询引擎可以在读取数据之前将部分查询条件推送到文件的元数据中，从而进一步减少所需的数据量。这在大型数据集上可以显著提高查询性能。

总的来说，如果你的数据量较小，性能要求不高，或者你对查询性能没有特别的要求，直接从 JSON 文件创建临时视图并执行 Spark SQL 查询是完全可行的。然而，在处理大规模数据时，将数据转换为 Parquet 格式可能是一个更好的选择，以提高性能和查询效率。
```
